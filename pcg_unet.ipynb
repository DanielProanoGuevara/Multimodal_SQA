{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import librosa\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import scipy.signal\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "\n",
    "from scipy.io import wavfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "import feature_extraction_lib as ftelib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and validation datasets\n",
    "train_df = pd.read_pickle('../train_physionet_2016.pkl')\n",
    "val_df = pd.read_pickle('../validation_physionet_2016.pkl')\n",
    "\n",
    "# Convert the loaded DataFrames to numpy arrays\n",
    "train_data = train_df[['Patient ID', 'Homomorphic',\n",
    "                       'CWT_Morl', 'CWT_Mexh', 'Hilbert_Env', 'Labels']].to_numpy()\n",
    "val_data = val_df[['Patient ID', 'Homomorphic', 'CWT_Morl',\n",
    "                   'CWT_Mexh', 'Hilbert_Env', 'Labels']].to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the entire dataset to create patches\n",
    "def process_dataset(data, patch_size, stride):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    for i in range(data.shape[0]):\n",
    "        features = np.stack(data[i, 1:5], axis=-1)\n",
    "        labels = data[i, 5]\n",
    "        features_patches, labels_patches = ftelib.create_patches(\n",
    "            features, labels, patch_size, stride)\n",
    "        all_features.append(features_patches)\n",
    "        all_labels.append(labels_patches)\n",
    "    all_features = np.concatenate(all_features, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    return all_features, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the Data Preparers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature creation\n",
    "BATCH_SIZE = 4\n",
    "patch_size = 64\n",
    "nch = 4\n",
    "stride = 8\n",
    "\n",
    "\n",
    "train_features, train_labels = process_dataset(train_data, patch_size, stride)\n",
    "val_features, val_labels = process_dataset(val_data, patch_size, stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, UpSampling1D, concatenate\n",
    "\n",
    "# TODO: provide u-net with one encoder layer only and suggest for them to\n",
    "# increase its size.\n",
    "def unet_pcg(nch, patch_size, dropout=0.0):\n",
    "    inputs = tf.keras.layers.Input(shape=(patch_size, nch))\n",
    "    conv1 = tf.keras.layers.Conv1D(8, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = tf.keras.layers.Conv1D(8, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
    "    pool1 = tf.keras.layers.Dropout(dropout)(pool1)\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv1D(16, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = tf.keras.layers.Conv1D(16, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
    "    pool2 = tf.keras.layers.Dropout(dropout)(pool2)\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv1D(32, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = tf.keras.layers.Conv1D(32, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv3)\n",
    "    pool3 = tf.keras.layers.Dropout(dropout)(pool3)\n",
    "\n",
    "    conv4 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(conv4)\n",
    "    pool4 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv4)\n",
    "    pool4 = tf.keras.layers.Dropout(dropout)(pool4)\n",
    "\n",
    "    conv5 = tf.keras.layers.Conv1D(128, 3, activation='relu', padding='same')(pool4)\n",
    "    conv5 = tf.keras.layers.Conv1D(128, 3, activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6_prep = tf.keras.layers.UpSampling1D(size=2)(conv5)\n",
    "\n",
    "    up6 = tf.keras.layers.concatenate([tf.keras.layers.Conv1D(64, 2, padding='same')(up6_prep), conv4], axis=2)\n",
    "    up6 = tf.keras.layers.Dropout(dropout)(up6)\n",
    "    conv6 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(up6)\n",
    "    conv6 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7_prep = tf.keras.layers.UpSampling1D(size=2)(conv6)\n",
    "\n",
    "    up7 = tf.keras.layers.concatenate([tf.keras.layers.Conv1D(64, 2, padding='same')(up7_prep), conv3], axis=2)\n",
    "    up7 = tf.keras.layers.Dropout(dropout)(up7)\n",
    "    conv7 = tf.keras.layers.Conv1D(32, 3, activation='relu', padding='same')(up7)\n",
    "    conv7 = tf.keras.layers.Conv1D(32, 3, activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8_prep = tf.keras.layers.UpSampling1D(size=2)(conv7)\n",
    "\n",
    "    up8 = tf.keras.layers.concatenate([tf.keras.layers.Conv1D(32, 2, padding='same')(up8_prep), conv2], axis=2)\n",
    "    up8 = tf.keras.layers.Dropout(dropout)(up8)\n",
    "    conv8 = tf.keras.layers.Conv1D(16, 3, activation='relu', padding='same')(up8)\n",
    "    conv8 = tf.keras.layers.Conv1D(16, 3, activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9_prep = tf.keras.layers.UpSampling1D(size=2)(conv8)\n",
    "\n",
    "    up9 = tf.keras.layers.concatenate([tf.keras.layers.Conv1D(8, 2, padding='same')(up9_prep), conv1], axis=2)\n",
    "    up9 = tf.keras.layers.Dropout(dropout)(up9)\n",
    "    conv9 = tf.keras.layers.Conv1D(8, 3, activation='relu', padding='same')(up9)\n",
    "    conv9 = tf.keras.layers.Conv1D(8, 3, activation='tanh', padding='same')(conv9)\n",
    "\n",
    "    conv10 = tf.keras.layers.Conv1D(4, 1, activation='softmax')(conv9)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[conv10])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '../pcg_unet_weights/checkpoint.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune hyperpararmeter, epochs, e optimizer\n",
    "# choose adequate metrics\n",
    "# loss crossentropy, others?\n",
    "EPOCHS = 15\n",
    "learning_rate = 1e-4\n",
    "model = unet_pcg(nch, patch_size=patch_size)\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy',\n",
    "                  metrics=['CategoricalAccuracy', 'Precision', 'Recall'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "16120/16120 [==============================] - 794s 49ms/step - loss: 0.3812 - categorical_accuracy: 0.8762 - precision: 0.9040 - recall: 0.8512 - val_loss: 0.3260 - val_categorical_accuracy: 0.8915 - val_precision: 0.9056 - val_recall: 0.8774\n",
      "Epoch 2/15\n",
      "16120/16120 [==============================] - 791s 49ms/step - loss: 0.2997 - categorical_accuracy: 0.9024 - precision: 0.9151 - recall: 0.8907 - val_loss: 0.3133 - val_categorical_accuracy: 0.8956 - val_precision: 0.9105 - val_recall: 0.8800\n",
      "Epoch 3/15\n",
      "16120/16120 [==============================] - 794s 49ms/step - loss: 0.2817 - categorical_accuracy: 0.9083 - precision: 0.9196 - recall: 0.8981 - val_loss: 0.3065 - val_categorical_accuracy: 0.8994 - val_precision: 0.9124 - val_recall: 0.8858\n",
      "Epoch 4/15\n",
      "16120/16120 [==============================] - 792s 49ms/step - loss: 0.2694 - categorical_accuracy: 0.9117 - precision: 0.9222 - recall: 0.9024 - val_loss: 0.3064 - val_categorical_accuracy: 0.8995 - val_precision: 0.9101 - val_recall: 0.8886\n",
      "Epoch 5/15\n",
      "16120/16120 [==============================] - 786s 49ms/step - loss: 0.2584 - categorical_accuracy: 0.9149 - precision: 0.9249 - recall: 0.9063 - val_loss: 0.3073 - val_categorical_accuracy: 0.8993 - val_precision: 0.9097 - val_recall: 0.8889\n",
      "Epoch 6/15\n",
      "16120/16120 [==============================] - 785s 49ms/step - loss: 0.2475 - categorical_accuracy: 0.9176 - precision: 0.9271 - recall: 0.9096 - val_loss: 0.3104 - val_categorical_accuracy: 0.8975 - val_precision: 0.9068 - val_recall: 0.8884\n",
      "Epoch 7/15\n",
      "16120/16120 [==============================] - 785s 49ms/step - loss: 0.2379 - categorical_accuracy: 0.9202 - precision: 0.9292 - recall: 0.9124 - val_loss: 0.3078 - val_categorical_accuracy: 0.8990 - val_precision: 0.9092 - val_recall: 0.8895\n",
      "Epoch 8/15\n",
      "16120/16120 [==============================] - 783s 49ms/step - loss: 0.2268 - categorical_accuracy: 0.9232 - precision: 0.9318 - recall: 0.9160 - val_loss: 0.3217 - val_categorical_accuracy: 0.8967 - val_precision: 0.9049 - val_recall: 0.8890\n",
      "Epoch 9/15\n",
      "16120/16120 [==============================] - 785s 49ms/step - loss: 0.2170 - categorical_accuracy: 0.9259 - precision: 0.9340 - recall: 0.9192 - val_loss: 0.3366 - val_categorical_accuracy: 0.8942 - val_precision: 0.9019 - val_recall: 0.8872\n",
      "Epoch 10/15\n",
      "16120/16120 [==============================] - 789s 49ms/step - loss: 0.2083 - categorical_accuracy: 0.9282 - precision: 0.9359 - recall: 0.9217 - val_loss: 0.3362 - val_categorical_accuracy: 0.8954 - val_precision: 0.9026 - val_recall: 0.8890\n",
      "Epoch 11/15\n",
      "16120/16120 [==============================] - 789s 49ms/step - loss: 0.1989 - categorical_accuracy: 0.9306 - precision: 0.9380 - recall: 0.9246 - val_loss: 0.3330 - val_categorical_accuracy: 0.8952 - val_precision: 0.9026 - val_recall: 0.8882\n",
      "Epoch 12/15\n",
      "16120/16120 [==============================] - 786s 49ms/step - loss: 0.1905 - categorical_accuracy: 0.9330 - precision: 0.9399 - recall: 0.9273 - val_loss: 0.3608 - val_categorical_accuracy: 0.8919 - val_precision: 0.8989 - val_recall: 0.8859\n",
      "Epoch 13/15\n",
      "16120/16120 [==============================] - 793s 49ms/step - loss: 0.1821 - categorical_accuracy: 0.9354 - precision: 0.9419 - recall: 0.9300 - val_loss: 0.3630 - val_categorical_accuracy: 0.8920 - val_precision: 0.8987 - val_recall: 0.8861\n",
      "Epoch 14/15\n",
      "16120/16120 [==============================] - 789s 49ms/step - loss: 0.1748 - categorical_accuracy: 0.9374 - precision: 0.9435 - recall: 0.9324 - val_loss: 0.3903 - val_categorical_accuracy: 0.8912 - val_precision: 0.8966 - val_recall: 0.8867\n",
      "Epoch 15/15\n",
      "16120/16120 [==============================] - 785s 49ms/step - loss: 0.1678 - categorical_accuracy: 0.9393 - precision: 0.9451 - recall: 0.9346 - val_loss: 0.3797 - val_categorical_accuracy: 0.8917 - val_precision: 0.8976 - val_recall: 0.8867\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True)\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    validation_data=(val_features, val_labels),\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    verbose=1,\n",
    "                    shuffle=True, callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference pipeline\n",
    "Collect the predictions of the U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions_train = model.predict(train_features)\n",
    "val_test = model.predict(val_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_original_data(patched_data, original_lengths, patch_size, stride):\n",
    "    \"\"\"\n",
    "    Reconstruct the original sequences from patched data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    patched_data : numpy.ndarray\n",
    "        Patched data array of shape (Num_Patches, Patch_Size, Num_Features).\n",
    "    original_lengths : list\n",
    "        List of original lengths for each patient sequence.\n",
    "    patch_size : int\n",
    "        The number of samples for each patch.\n",
    "    stride : int\n",
    "        The number of samples to stride between patches.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    reconstructed_data : list of numpy.ndarray\n",
    "        List containing the reconstructed data for each patient.\n",
    "    \"\"\"\n",
    "    reconstructed_data = []\n",
    "    current_idx = 0\n",
    "\n",
    "    for original_length in original_lengths:\n",
    "        # Initialize arrays to hold the reconstructed sequence and overlap count\n",
    "        reconstructed = np.zeros((original_length, patched_data.shape[-1]))\n",
    "        overlap_count = np.zeros(original_length)\n",
    "\n",
    "        num_patches = int(np.floor((original_length - patch_size) / stride)) + 1\n",
    "        adjusted_stride_samples = (\n",
    "            (original_length - patch_size) / (num_patches - 1)\n",
    "            if num_patches > 1 else stride\n",
    "        )\n",
    "        adjusted_stride_samples = int(round(adjusted_stride_samples))\n",
    "\n",
    "        # Iterate over patches and reconstruct the sequence\n",
    "        for i in range(num_patches):\n",
    "            start_idx = i * adjusted_stride_samples\n",
    "            end_idx = min(start_idx + patch_size, original_length)\n",
    "\n",
    "            reconstructed[start_idx:end_idx] += patched_data[current_idx, :end_idx - start_idx, :]\n",
    "            overlap_count[start_idx:end_idx] += 1\n",
    "\n",
    "            current_idx += 1\n",
    "\n",
    "        # Average the overlapping regions\n",
    "        reconstructed /= np.maximum(overlap_count[:, None], 1)\n",
    "        reconstructed_data.append(reconstructed)\n",
    "\n",
    "    return reconstructed_data\n",
    "\n",
    "\n",
    "original_lengths = [len(seq) for seq in val_data[:, 1]]  # Get original lengths from validation data\n",
    "reconstructed_labels = reconstruct_original_data(val_test, original_lengths, patch_size, stride)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_temporal_modelling(seq, num_states=4):\n",
    "  for t in range(1, len(seq)):\n",
    "    if seq[t] != seq[t-1] and seq[t] != ((seq[t-1] + 1) % num_states):\n",
    "      seq[t] = seq[t-1]\n",
    "  return seq\n",
    "\n",
    "test_seq = np.array([1, 1, 1, 2, 1, 2, 2, 3, 3, 0, 3])\n",
    "exp_seq = np.array([1, 1, 1, 2, 2, 2, 2, 3, 3, 0, 0])\n",
    "\n",
    "max_temporal_modelling(test_seq)\n",
    "assert np.all(exp_seq == test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation - metrics Schmidt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_one_hot_encoding(one_hot_encoded_data, desired_order=[0, 1, 2, 3]):\n",
    "    \"\"\"\n",
    "    Reverse the one-hot encoding to get the original labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    one_hot_encoded_data : numpy.ndarray\n",
    "        One-hot encoded data of shape (Num_Samples, Num_Classes).\n",
    "    desired_order : list\n",
    "        List representing the label order used during one-hot encoding.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    labels : numpy.ndarray\n",
    "        Array of decoded labels of shape (Num_Samples,).\n",
    "    \"\"\"\n",
    "    # Use argmax to find the index of the maximum value in each one-hot encoded row\n",
    "    label_indices = np.argmax(one_hot_encoded_data, axis=1)\n",
    "    labels = np.array([desired_order[idx] for idx in label_indices])\n",
    "    return labels\n",
    "\n",
    "\n",
    "original_lengths = [len(seq) for seq in val_data[:, 1]]  # Get original lengths from validation data\n",
    "reconstructed_labels = reconstruct_original_data(val_test, original_lengths, patch_size, stride)\n",
    "\n",
    "pred_labels = [reverse_one_hot_encoding(pred) for pred in reconstructed_labels]\n",
    "\n",
    "prediction_labels = copy.deepcopy(pred_labels)\n",
    "\n",
    "ground_truth = [reverse_one_hot_encoding(pred) for pred in val_data[:, 5]]\n",
    "\n",
    "predictions = np.array([max_temporal_modelling(prediction) for prediction in prediction_labels], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "def extract_state_runs(labels, desired_states):\n",
    "    \"\"\"\n",
    "    Extract continuous runs of the desired states from labels.\n",
    "\n",
    "    Args:\n",
    "        labels: numpy array of labels.\n",
    "        desired_states: set of desired state values.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries with keys:\n",
    "            'start': start index of the run\n",
    "            'end': end index of the run (inclusive)\n",
    "            'midpoint': midpoint index of the run\n",
    "            'state': the state value (0 or 2)\n",
    "    \"\"\"\n",
    "    # Ensure labels is a 1D array\n",
    "    labels = np.asarray(labels).flatten()\n",
    "\n",
    "    runs = []\n",
    "    N = len(labels)\n",
    "    in_run = False\n",
    "    run_start = 0\n",
    "    run_state = None\n",
    "\n",
    "    for i in range(N):\n",
    "        label_i = labels[i]\n",
    "        # If label_i is an array (e.g., from a structured array), extract scalar\n",
    "        if isinstance(label_i, np.ndarray):\n",
    "            label_i = label_i.item()\n",
    "        if label_i in desired_states:\n",
    "            if not in_run:\n",
    "                # Start of a new run\n",
    "                in_run = True\n",
    "                run_start = i\n",
    "                run_state = label_i\n",
    "        else:\n",
    "            if in_run:\n",
    "                # End of the run\n",
    "                run_end = i - 1\n",
    "                midpoint = (run_start + run_end) // 2\n",
    "                runs.append({\n",
    "                    'start': run_start,\n",
    "                    'end': run_end,\n",
    "                    'midpoint': midpoint,\n",
    "                    'state': run_state\n",
    "                })\n",
    "                in_run = False\n",
    "                run_state = None\n",
    "    # Check if we're still in a run at the end\n",
    "    if in_run:\n",
    "        run_end = N - 1\n",
    "        midpoint = (run_start + run_end) // 2\n",
    "        runs.append({\n",
    "            'start': run_start,\n",
    "            'end': run_end,\n",
    "            'midpoint': midpoint,\n",
    "            'state': run_state\n",
    "        })\n",
    "    return runs\n",
    "\n",
    "def compute_ppv_sensitivity(ground_truth, predictions, sample_rate, threshold=60e-3):\n",
    "    \"\"\"\n",
    "    Compute PPV and sensitivity for states 0 and 2.\n",
    "\n",
    "    Args:\n",
    "        ground_truth: numpy array of ground truth labels.\n",
    "        predictions: numpy array of predicted labels.\n",
    "        sample_rate: sampling rate in Hz.\n",
    "\n",
    "    Returns:\n",
    "        ppv: Positive Predictive Value.\n",
    "        sensitivity: Sensitivity (Recall).\n",
    "    \"\"\"\n",
    "    # Ensure ground_truth and predictions are 1D arrays\n",
    "    ground_truth = np.asarray(ground_truth).flatten()\n",
    "    predictions = np.asarray(predictions).flatten()\n",
    "\n",
    "    # Desired states\n",
    "    desired_states = {0, 2}\n",
    "\n",
    "    # Maximum distance in samples (treshold in seconds vs fs)\n",
    "    max_distance_samples = int(threshold * sample_rate)\n",
    "\n",
    "    # Extract runs from ground truth and predictions\n",
    "    gt_runs = extract_state_runs(ground_truth, desired_states)\n",
    "    pred_runs = extract_state_runs(predictions, desired_states)\n",
    "\n",
    "    # Get midpoints and states\n",
    "    gt_midpoints = np.array([run['midpoint'] for run in gt_runs])\n",
    "    gt_states = np.array([run['state'] for run in gt_runs])\n",
    "\n",
    "    pred_midpoints = np.array([run['midpoint'] for run in pred_runs])\n",
    "    pred_states = np.array([run['state'] for run in pred_runs])\n",
    "\n",
    "    # Initialize matches\n",
    "    matched_gt_indices = set()\n",
    "    matched_pred_indices = set()\n",
    "\n",
    "    # Build potential matches\n",
    "    potential_matches = []\n",
    "    for i, (p_mid, p_state) in enumerate(zip(pred_midpoints, pred_states)):\n",
    "        for j, (gt_mid, gt_state) in enumerate(zip(gt_midpoints, gt_states)):\n",
    "            if gt_state == p_state:\n",
    "                distance = abs(p_mid - gt_mid)\n",
    "                if distance <= max_distance_samples:\n",
    "                    potential_matches.append((i, j, distance))\n",
    "\n",
    "    # Sort potential matches by distance\n",
    "    potential_matches.sort(key=lambda x: x[2])\n",
    "\n",
    "    # Perform matching\n",
    "    TP = 0\n",
    "    for i, j, d in potential_matches:\n",
    "        if i not in matched_pred_indices and j not in matched_gt_indices:\n",
    "            matched_pred_indices.add(i)\n",
    "            matched_gt_indices.add(j)\n",
    "            TP += 1\n",
    "\n",
    "    # Compute FP and FN\n",
    "    total_pred = len(pred_midpoints)\n",
    "    total_gt = len(gt_midpoints)\n",
    "    FP = total_pred - len(matched_pred_indices)\n",
    "    FN = total_gt - len(matched_gt_indices)\n",
    "\n",
    "    # Compute PPV and Sensitivity\n",
    "    ppv = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "    return ppv, sensitivity\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have numpy arrays `ground_truth_labels` and `predicted_labels`, and `sample_rate`.\n",
    "\n",
    "# ground_truth_labels = np.array([...])  # Your ground truth labels\n",
    "# predicted_labels = np.array([...])     # Your predicted labels\n",
    "# sample_rate = 1000  # Example sample rate in Hz\n",
    "\n",
    "\n",
    "def compute_schmidt_metrics(ground_truth, sequences, sample_rate):\n",
    "  ppvs, sensitivities, accuracies = [], [], []\n",
    "  for i in range(len(ground_truth)):\n",
    "    ppv, sensitivity = compute_ppv_sensitivity(ground_truth[i],\n",
    "                                               sequences[i],\n",
    "                                               50)\n",
    "    ppvs.append(ppv)\n",
    "    sensitivities.append(sensitivity)\n",
    "    accuracies.append(accuracy_score(ground_truth[i], sequences[i]))\n",
    "  return np.array(ppvs), np.array(sensitivities), np.array(accuracies)\n",
    "\n",
    "\n",
    "ppv, sens, acc = compute_schmidt_metrics(ground_truth, predictions, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def visualize_predictions(ground_truth, seqs, idx):\n",
    "\n",
    "  # Define the window width in terms of seconds and convert to the corresponding sample range\n",
    "  window_duration = 0.1  # 0.06 ms in seconds\n",
    "  sample_interval = 1 / 50  # Time per sample in seconds (20 ms per sample at 50 Hz)\n",
    "\n",
    "  # Calculate the equivalent width in terms of sample indices (will be <1)\n",
    "  window_width_samples = window_duration / sample_interval\n",
    "\n",
    "  # Create the plot\n",
    "  plt.figure(figsize=(24, 6))\n",
    "\n",
    "  # Plot ground truth and predictions with discrete markers and dotted lines\n",
    "  plt.plot(ground_truth[idx], 'o--', label='Ground Truth', markersize=6)\n",
    "  plt.plot(seqs[idx], 'o--', color='red', label='Predictions', markersize=6)\n",
    "\n",
    "\n",
    "  # Set labels and legend\n",
    "  plt.title(f'Signal at idx {idx} with 0.06 ms reference window')\n",
    "  plt.xlabel('Sample Index')\n",
    "  plt.ylabel('Amplitude')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "visualize_predictions(ground_truth, predictions, idx)\n",
    "ppv[idx], sens[idx], acc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 13\n",
    "visualize_predictions(ground_truth, predictions, idx)\n",
    "ppv[idx], sens[idx], acc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 44\n",
    "visualize_predictions(ground_truth, predictions, idx)\n",
    "ppv[idx], sens[idx], acc[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bins = np.histogram(ppv)\n",
    "plt.stairs(counts, bins)\n",
    "plt.title('Positive Predictive Values')\n",
    "plt.xlabel('Incidence')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bins = np.histogram(sens)\n",
    "plt.stairs(counts, bins)\n",
    "plt.title('Sensibility')\n",
    "plt.xlabel('Incidence')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bins = np.histogram(acc)\n",
    "plt.stairs(counts, bins)\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Incidence')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signalProcessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
