{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import librosa\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import scipy.signal\n",
    "import re\n",
    "\n",
    "import pickle\n",
    "\n",
    "from scipy.io import wavfile\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train dataset\n",
    "train_df = pd.read_pickle('../train_physionet_2016.pkl')\n",
    "val_df = pd.read_pickle('../validation_physionet_2016.pkl')\n",
    "test_df = pd.read_pickle('../test_physionet_2016.pkl')\n",
    "\n",
    "# Convert the loaded DataFrame to the desired numpy format\n",
    "train_data = []\n",
    "for index, row in train_df.iterrows():\n",
    "    patient_id = row['Patient ID']\n",
    "    homomorphic = row['Homomorphic']\n",
    "    cwt_morl = row['CWT_Morl']\n",
    "    cwt_mexh = row['CWT_Mexh']\n",
    "    hilbert_env = row['Hilbert_Env']\n",
    "    labels = row['Labels']\n",
    "\n",
    "    # Append each patient's data as a tuple to the train_data list\n",
    "    train_data.append([patient_id, homomorphic, cwt_morl, cwt_mexh, hilbert_env, labels])\n",
    "\n",
    "val_data = []\n",
    "for index, row in val_df.iterrows():\n",
    "    patient_id = row['Patient ID']\n",
    "    homomorphic = row['Homomorphic']\n",
    "    cwt_morl = row['CWT_Morl']\n",
    "    cwt_mexh = row['CWT_Mexh']\n",
    "    hilbert_env = row['Hilbert_Env']\n",
    "    labels = row['Labels']\n",
    "\n",
    "    # Append each patient's data as a tuple to the train_data list\n",
    "    val_data.append([patient_id, homomorphic, cwt_morl, cwt_mexh, hilbert_env, labels])\n",
    "\n",
    "test_data = []\n",
    "for index, row in val_df.iterrows():\n",
    "    patient_id = row['Patient ID']\n",
    "    homomorphic = row['Homomorphic']\n",
    "    cwt_morl = row['CWT_Morl']\n",
    "    cwt_mexh = row['CWT_Mexh']\n",
    "    hilbert_env = row['Hilbert_Env']\n",
    "    labels = row['Labels']\n",
    "\n",
    "    # Append each patient's data as a tuple to the train_data list\n",
    "    test_data.append([patient_id, homomorphic, cwt_morl, cwt_mexh, hilbert_env, labels])\n",
    "\n",
    "# Convert train_data to a numpy array with dtype=object to handle mixed types\n",
    "train = np.array(train_data, dtype=object)\n",
    "val = np.array(val_data, dtype=object)\n",
    "test = np.array(test_data, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Smaller than patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_smaller_than_patch(features, patch_size):\n",
    "    # Remove sounds shorter than patch size and return their indices\n",
    "    return np.array([j for j in range(len(features)) if len(features[j]) >= patch_size], dtype=int)\n",
    "\n",
    "patch_size = 64\n",
    "nch = 4\n",
    "stride = 32\n",
    "\n",
    "# Ensure indices are integers and apply them correctly to filter the datasets\n",
    "train_indices = filter_smaller_than_patch(train[:,2], patch_size)\n",
    "val_indices = filter_smaller_than_patch(val[:,2], patch_size)\n",
    "test_indices = filter_smaller_than_patch(test[:,2], patch_size)\n",
    "\n",
    "train = train[train_indices, ...]\n",
    "val = val[val_indices, ...]\n",
    "test = test[test_indices, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCGDataPreparer:\n",
    "    def __init__(self, patch_size: int , stride: int, number_channels: int=4, num_states: int=4):\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.number_channels = number_channels\n",
    "        self.num_states = num_states\n",
    "        self.features = None\n",
    "        self.labels = None\n",
    "\n",
    "    def _compute_pcg_patches(self, sound, label):\n",
    "        #TODO: ask them to implement this\n",
    "        num_samples = len(sound)\n",
    "        # TODO: they should complete this for\n",
    "        num_windows = int((num_samples - self.patch_size) / self.stride) + 1\n",
    "        for window_idx in range(num_windows):\n",
    "            patch_start = window_idx * self.stride\n",
    "            yield sound[patch_start:patch_start + self.patch_size, :],  label[patch_start: patch_start + self.patch_size, :]\n",
    "\n",
    "        window_remain = num_samples - self.patch_size\n",
    "        if window_remain % self.stride > 0:\n",
    "          yield sound[window_remain:, :], label[window_remain:, :]\n",
    "\n",
    "    def set_features_and_labels(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        num_observations = len(self.features)\n",
    "        total_windows = 0\n",
    "        for obs in features:\n",
    "          num_samples = len(features)\n",
    "          num_windows = int((num_samples - self.patch_size) / self.stride) + 1\n",
    "          window_remain = num_samples - self.patch_size\n",
    "          if window_remain % self.stride > 0:\n",
    "              num_windows += 1\n",
    "          total_windows += num_windows\n",
    "        self.num_steps = total_windows\n",
    "\n",
    "    def __call__(self):\n",
    "        num_observations = len(self.labels)\n",
    "        for obs_idx in range(num_observations):\n",
    "            features = tf.stack(self.features[obs_idx], axis=1) # np.column_stack\n",
    "            labels = self.labels[obs_idx]\n",
    "            for s,y in (self._compute_pcg_patches(features, labels)):\n",
    "              yield s, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the Data Preparers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 64\n",
    "nch = 2\n",
    "stride = 32\n",
    "train_dp = PCGDataPreparer(patch_size=patch_size,\n",
    "                     number_channels=nch,\n",
    "                     stride=stride,\n",
    "                     num_states=4)\n",
    "train_dp.set_features_and_labels(train[:, [1,2,3,4]], train[:, 5])\n",
    "\n",
    "val_dp = PCGDataPreparer(patch_size=patch_size,\n",
    "                     number_channels=nch,\n",
    "                     stride=stride,\n",
    "                     num_states=4)\n",
    "val_dp.set_features_and_labels(val[:, [1,2,3,4]], val[:, 5])\n",
    "\n",
    "test_dp = PCGDataPreparer(patch_size=patch_size,\n",
    "                     number_channels=nch,\n",
    "                     stride=stride,\n",
    "                     num_states=4)\n",
    "test_dp.set_features_and_labels(test[:, [1,2,3,4]], test[:, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Dataset and caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "def get_data_from_generator(*, data_processor, batch_size, patch_size, number_channels, number_classes, trainable=True):\n",
    "    data = tf.data.Dataset.from_generator(data_processor,\n",
    "                                          output_signature=(\n",
    "                                              tf.TensorSpec(shape=(patch_size, number_channels), dtype=tf.float32),\n",
    "                                              tf.TensorSpec(shape=(patch_size, number_classes), dtype=tf.float32))\n",
    "                                          )\n",
    "    if trainable:\n",
    "        data = data.shuffle(5000, reshuffle_each_iteration=True)\n",
    "        data.cache()\n",
    "    data = data.batch(batch_size)\n",
    "    data = data.prefetch(tf.data.AUTOTUNE)\n",
    "    return data\n",
    "\n",
    "train_dataset = get_data_from_generator(data_processor=train_dp,\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                patch_size=patch_size,\n",
    "                                                number_channels=nch,\n",
    "                                                number_classes=4,\n",
    "                                                trainable=True)\n",
    "\n",
    "\n",
    "val_dataset = get_data_from_generator(data_processor=val_dp,\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                patch_size=patch_size,\n",
    "                                                number_channels=nch,\n",
    "                                                number_classes=4,\n",
    "                                                trainable=False)\n",
    "\n",
    "test_dataset = get_data_from_generator(data_processor=test_dp,\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                patch_size=patch_size,\n",
    "                                                number_channels=nch,\n",
    "                                                number_classes=4,\n",
    "                                                trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, UpSampling1D, concatenate\n",
    "\n",
    "# TODO: provide u-net with one encoder layer only and suggest for them to\n",
    "# increase its size.\n",
    "def unet_pcg(nch, patch_size, dropout=0.0):\n",
    "    inputs = tf.keras.layers.Input(shape=(patch_size, nch))\n",
    "    conv1 = tf.keras.layers.Conv1D(8, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = tf.keras.layers.Conv1D(8, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
    "    pool1 = tf.keras.layers.Dropout(dropout)(pool1)\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv1D(16, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = tf.keras.layers.Conv1D(16, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
    "    pool2 = tf.keras.layers.Dropout(dropout)(pool2)\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv1D(32, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = tf.keras.layers.Conv1D(32, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv3)\n",
    "    pool3 = tf.keras.layers.Dropout(dropout)(pool3)\n",
    "\n",
    "    conv4 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(conv4)\n",
    "    pool4 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv4)\n",
    "    pool4 = tf.keras.layers.Dropout(dropout)(pool4)\n",
    "\n",
    "    conv5 = tf.keras.layers.Conv1D(128, 3, activation='relu', padding='same')(pool4)\n",
    "    conv5 = tf.keras.layers.Conv1D(128, 3, activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6_prep = tf.keras.layers.UpSampling1D(size=2)(conv5)\n",
    "\n",
    "    up6 = tf.keras.layers.concatenate([tf.keras.layers.Conv1D(64, 2, padding='same')(up6_prep), conv4], axis=2)\n",
    "    up6 = tf.keras.layers.Dropout(dropout)(up6)\n",
    "    conv6 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(up6)\n",
    "    conv6 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7_prep = tf.keras.layers.UpSampling1D(size=2)(conv6)\n",
    "\n",
    "    up7 = tf.keras.layers.concatenate([tf.keras.layers.Conv1D(64, 2, padding='same')(up7_prep), conv3], axis=2)\n",
    "    up7 = tf.keras.layers.Dropout(dropout)(up7)\n",
    "    conv7 = tf.keras.layers.Conv1D(32, 3, activation='relu', padding='same')(up7)\n",
    "    conv7 = tf.keras.layers.Conv1D(32, 3, activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8_prep = tf.keras.layers.UpSampling1D(size=2)(conv7)\n",
    "\n",
    "    up8 = tf.keras.layers.concatenate([tf.keras.layers.Conv1D(32, 2, padding='same')(up8_prep), conv2], axis=2)\n",
    "    up8 = tf.keras.layers.Dropout(dropout)(up8)\n",
    "    conv8 = tf.keras.layers.Conv1D(16, 3, activation='relu', padding='same')(up8)\n",
    "    conv8 = tf.keras.layers.Conv1D(16, 3, activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9_prep = tf.keras.layers.UpSampling1D(size=2)(conv8)\n",
    "\n",
    "    up9 = tf.keras.layers.concatenate([tf.keras.layers.Conv1D(8, 2, padding='same')(up9_prep), conv1], axis=2)\n",
    "    up9 = tf.keras.layers.Dropout(dropout)(up9)\n",
    "    conv9 = tf.keras.layers.Conv1D(8, 3, activation='relu', padding='same')(up9)\n",
    "    conv9 = tf.keras.layers.Conv1D(8, 3, activation='tanh', padding='same')(conv9)\n",
    "\n",
    "    conv10 = tf.keras.layers.Conv1D(4, 1, activation='softmax')(conv9)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[conv10])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\engine\\training.py\", line 998, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\engine\\training.py\", line 1092, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 577, in update_state\n        self.build(y_pred, y_true)\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 483, in build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 631, in _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 631, in <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 650, in _get_metric_object\n        metric_obj = metrics_mod.get(metric)\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\metrics\\__init__.py\", line 181, in get\n        return deserialize(str(identifier))\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\metrics\\__init__.py\", line 136, in deserialize\n        return deserialize_keras_object(\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 769, in deserialize_keras_object\n        raise ValueError(\n\n    ValueError: Unknown metric function: precision. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m                   metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39mcheckpoint_path, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 11\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# steps_per_epoch = int(np.floor(train_dp.num_steps / BATCH_SIZE)),\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(checkpoint_path)\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filee1h1hp3n.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\engine\\training.py\", line 998, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\engine\\training.py\", line 1092, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 577, in update_state\n        self.build(y_pred, y_true)\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 483, in build\n        self._metrics = tf.__internal__.nest.map_structure_up_to(\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 631, in _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 631, in <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 650, in _get_metric_object\n        metric_obj = metrics_mod.get(metric)\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\metrics\\__init__.py\", line 181, in get\n        return deserialize(str(identifier))\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\metrics\\__init__.py\", line 136, in deserialize\n        return deserialize_keras_object(\n    File \"c:\\Users\\danie\\anaconda3\\envs\\signalProcessing\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 769, in deserialize_keras_object\n        raise ValueError(\n\n    ValueError: Unknown metric function: precision. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n"
     ]
    }
   ],
   "source": [
    "# tune hyperpararmeter, epochs, e optimizer\n",
    "# choose adequate metrics\n",
    "# loss crossentropy, others?\n",
    "checkpoint_path = './unet_weights/checkpoint.keras'\n",
    "EPOCHS = 10\n",
    "learning_rate = 1e-4\n",
    "model = unet_pcg(nch, patch_size)\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy',\n",
    "                  metrics=['categorical_accuracy', 'precision', 'recall'])\n",
    "model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True)\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data=val_dataset,\n",
    "                    epochs=EPOCHS,\n",
    "                    # steps_per_epoch = int(np.floor(train_dp.num_steps / BATCH_SIZE)),\n",
    "                    verbose=1,\n",
    "                    shuffle=True, callbacks=[model_checkpoint])\n",
    "\n",
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signalProcessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
