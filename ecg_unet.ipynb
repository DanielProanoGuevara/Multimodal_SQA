{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "import copy\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, UpSampling1D, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import feature_extraction_lib as ftelib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and validation datasets\n",
    "train_df = pd.read_pickle('../train_ludb.pkl')\n",
    "val_df = pd.read_pickle('../validation_ludb.pkl')\n",
    "\n",
    "# Convert the loaded Dataframes to numpy arrays\n",
    "train_data = train_df[['Patient ID', 'Hilbert', 'Shannon', 'Homomorphic', 'Hamming', 'Labels']].to_numpy()\n",
    "val_data = val_df[['Patient ID', 'Hilbert', 'Shannon', 'Homomorphic', 'Hamming', 'Labels']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare data pepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "patch_size = 64\n",
    "nch = 4\n",
    "stride = 8\n",
    "\n",
    "train_features, train_labels = ftelib.process_dataset(train_data, patch_size, stride)\n",
    "val_features, val_labels = ftelib.process_dataset(val_data, patch_size, stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_ecg(nch, patch_size, dropout=0.05):\n",
    "    inputs = tf.keras.layers.Input(shape=(patch_size, nch))\n",
    "    conv1 = tf.keras.layers.Conv1D(8, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = tf.keras.layers.Conv1D(8, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
    "    pool1 = tf.keras.layers.Dropout(dropout)(pool1)\n",
    "\n",
    "    conv2 = tf.keras.layers.Conv1D(16, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = tf.keras.layers.Conv1D(16, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
    "    pool2 = tf.keras.layers.Dropout(dropout)(pool2)\n",
    "\n",
    "    conv3 = tf.keras.layers.Conv1D(32, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = tf.keras.layers.Conv1D(32, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv3)\n",
    "    pool3 = tf.keras.layers.Dropout(dropout)(pool3)\n",
    "\n",
    "    conv4 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(conv4)\n",
    "    pool4 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv4)\n",
    "    pool4 = tf.keras.layers.Dropout(dropout)(pool4)\n",
    "\n",
    "    conv5 = tf.keras.layers.Conv1D(128, 3, activation='relu', padding='same')(pool4)\n",
    "    conv5 = tf.keras.layers.Conv1D(128, 3, activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6_prep = tf.keras.layers.UpSampling1D(size=2)(conv5)\n",
    "\n",
    "    up6 = tf.keras.layers.concatenate([tf.keras.layers.Conv1D(64, 2, padding='same')(up6_prep), conv4], axis=2)\n",
    "    up6 = tf.keras.layers.Dropout(dropout)(up6)\n",
    "    conv6 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(up6)\n",
    "    conv6 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7_prep = tf.keras.layers.UpSampling1D(size=2)(conv6)\n",
    "\n",
    "    up7 = tf.keras.layers.concatenate([tf.keras.layers.Conv1D(64, 2, padding='same')(up7_prep), conv3], axis=2)\n",
    "    up7 = tf.keras.layers.Dropout(dropout)(up7)\n",
    "    conv7 = tf.keras.layers.Conv1D(32, 3, activation='relu', padding='same')(up7)\n",
    "    conv7 = tf.keras.layers.Conv1D(32, 3, activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8_prep = tf.keras.layers.UpSampling1D(size=2)(conv7)\n",
    "\n",
    "    up8 = tf.keras.layers.concatenate([tf.keras.layers.Conv1D(32, 2, padding='same')(up8_prep), conv2], axis=2)\n",
    "    up8 = tf.keras.layers.Dropout(dropout)(up8)\n",
    "    conv8 = tf.keras.layers.Conv1D(16, 3, activation='relu', padding='same')(up8)\n",
    "    conv8 = tf.keras.layers.Conv1D(16, 3, activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9_prep = tf.keras.layers.UpSampling1D(size=2)(conv8)\n",
    "\n",
    "    up9 = tf.keras.layers.concatenate([tf.keras.layers.Conv1D(8, 2, padding='same')(up9_prep), conv1], axis=2)\n",
    "    up9 = tf.keras.layers.Dropout(dropout)(up9)\n",
    "    conv9 = tf.keras.layers.Conv1D(8, 3, activation='relu', padding='same')(up9)\n",
    "    conv9 = tf.keras.layers.Conv1D(8, 3, activation='tanh', padding='same')(conv9)\n",
    "\n",
    "    conv10 = tf.keras.layers.Conv1D(4, 1, activation='softmax')(conv9)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[conv10])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '../ecg_unet_weights/checkpoint.keras'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "learning_rate = 1e-4\n",
    "model = unet_ecg(nch, patch_size=patch_size)\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy',\n",
    "              metrics=['CategoricalAccuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True)\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    validation_data=(val_features, val_labels),\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    verbose=1,\n",
    "                    shuffle=True, callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Pipeline\n",
    "\n",
    "Collect the predicions of the U-NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = model.predict(train_features)\n",
    "predictions_val = model.predict(val_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_original_data(patched_data, original_lengths, patch_size, stride):\n",
    "    \"\"\"\n",
    "    Reconstruct the original sequences from patched data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    patched_data : numpy.ndarray\n",
    "        Patched data array of shape (Num_Patches, Patch_Size, Num_Features).\n",
    "    original_lengths : list\n",
    "        List of original lengths for each patient sequence.\n",
    "    patch_size : int\n",
    "        The number of samples for each patch.\n",
    "    stride : int\n",
    "        The number of samples to stride between patches.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    reconstructed_data : list of numpy.ndarray\n",
    "        List containing the reconstructed data for each patient.\n",
    "    \"\"\"\n",
    "    reconstructed_data = []\n",
    "    current_idx = 0\n",
    "\n",
    "    for original_length in original_lengths:\n",
    "        # Initialize arrays to hold the reconstructed sequence and overlap count\n",
    "        reconstructed = np.zeros((original_length, patched_data.shape[-1]))\n",
    "        overlap_count = np.zeros(original_length)\n",
    "\n",
    "        num_patches = int(np.floor((original_length - patch_size) / stride)) + 1\n",
    "        adjusted_stride_samples = (\n",
    "            (original_length - patch_size) / (num_patches - 1)\n",
    "            if num_patches > 1 else stride\n",
    "        )\n",
    "        adjusted_stride_samples = int(round(adjusted_stride_samples))\n",
    "\n",
    "        # Iterate over patches and reconstruct the sequence\n",
    "        for i in range(num_patches):\n",
    "            start_idx = i * adjusted_stride_samples\n",
    "            end_idx = min(start_idx + patch_size, original_length)\n",
    "\n",
    "            reconstructed[start_idx:end_idx] += patched_data[current_idx, :end_idx - start_idx, :]\n",
    "            overlap_count[start_idx:end_idx] += 1\n",
    "\n",
    "            current_idx += 1\n",
    "\n",
    "        # Average the overlapping regions\n",
    "        reconstructed /= np.maximum(overlap_count[:, None], 1)\n",
    "        reconstructed_data.append(reconstructed)\n",
    "\n",
    "    return reconstructed_data\n",
    "\n",
    "\n",
    "original_lengths = [len(seq) for seq in val_data[:, 1]]  # Get original lengths from validation data\n",
    "reconstructed_labels = reconstruct_original_data(predictions_val, original_lengths, patch_size, stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Schmidt Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_one_hot_encoding(one_hot_encoded_data, desired_order=[0, 1, 2, 3]):\n",
    "    \"\"\"\n",
    "    Reverse the one-hot encoding to get the original labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    one_hot_encoded_data : numpy.ndarray\n",
    "        One-hot encoded data of shape (Num_Samples, Num_Classes).\n",
    "    desired_order : list\n",
    "        List representing the label order used during one-hot encoding.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    labels : numpy.ndarray\n",
    "        Array of decoded labels of shape (Num_Samples,).\n",
    "    \"\"\"\n",
    "    # Use argmax to find the index of the maximum value in each one-hot encoded row\n",
    "    label_indices = np.argmax(one_hot_encoded_data, axis=1)\n",
    "    labels = np.array([desired_order[idx] for idx in label_indices])\n",
    "    return labels\n",
    "\n",
    "\n",
    "original_lengths = [len(seq) for seq in val_data[:, 1]]  # Get original lengths from validation data\n",
    "reconstructed_labels = reconstruct_original_data(val_test, original_lengths, patch_size, stride)\n",
    "\n",
    "pred_labels = [reverse_one_hot_encoding(pred) for pred in reconstructed_labels]\n",
    "\n",
    "prediction_labels = copy.deepcopy(pred_labels)\n",
    "\n",
    "ground_truth = [reverse_one_hot_encoding(pred) for pred in val_data[:, 5]]\n",
    "\n",
    "predictions = np.array([prediction for prediction in prediction_labels], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_state_runs(labels, desired_states):\n",
    "    \"\"\"\n",
    "    Extract continuous runs of the desired states from labels.\n",
    "\n",
    "    Args:\n",
    "        labels: numpy array of labels.\n",
    "        desired_states: set of desired state values.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries with keys:\n",
    "            'start': start index of the run\n",
    "            'end': end index of the run (inclusive)\n",
    "            'midpoint': midpoint index of the run\n",
    "            'state': the state value (0 or 2)\n",
    "    \"\"\"\n",
    "    # Ensure labels is a 1D array\n",
    "    labels = np.asarray(labels).flatten()\n",
    "\n",
    "    runs = []\n",
    "    N = len(labels)\n",
    "    in_run = False\n",
    "    run_start = 0\n",
    "    run_state = None\n",
    "\n",
    "    for i in range(N):\n",
    "        label_i = labels[i]\n",
    "        # If label_i is an array (e.g., from a structured array), extract scalar\n",
    "        if isinstance(label_i, np.ndarray):\n",
    "            label_i = label_i.item()\n",
    "        if label_i in desired_states:\n",
    "            if not in_run:\n",
    "                # Start of a new run\n",
    "                in_run = True\n",
    "                run_start = i\n",
    "                run_state = label_i\n",
    "        else:\n",
    "            if in_run:\n",
    "                # End of the run\n",
    "                run_end = i - 1\n",
    "                midpoint = (run_start + run_end) // 2\n",
    "                runs.append({\n",
    "                    'start': run_start,\n",
    "                    'end': run_end,\n",
    "                    'midpoint': midpoint,\n",
    "                    'state': run_state\n",
    "                })\n",
    "                in_run = False\n",
    "                run_state = None\n",
    "    # Check if we're still in a run at the end\n",
    "    if in_run:\n",
    "        run_end = N - 1\n",
    "        midpoint = (run_start + run_end) // 2\n",
    "        runs.append({\n",
    "            'start': run_start,\n",
    "            'end': run_end,\n",
    "            'midpoint': midpoint,\n",
    "            'state': run_state\n",
    "        })\n",
    "    return runs\n",
    "\n",
    "def compute_ppv_sensitivity(ground_truth, predictions, sample_rate, threshold=60e-3):\n",
    "    \"\"\"\n",
    "    Compute PPV and sensitivity for states 0 and 2.\n",
    "\n",
    "    Args:\n",
    "        ground_truth: numpy array of ground truth labels.\n",
    "        predictions: numpy array of predicted labels.\n",
    "        sample_rate: sampling rate in Hz.\n",
    "\n",
    "    Returns:\n",
    "        ppv: Positive Predictive Value.\n",
    "        sensitivity: Sensitivity (Recall).\n",
    "    \"\"\"\n",
    "    # Ensure ground_truth and predictions are 1D arrays\n",
    "    ground_truth = np.asarray(ground_truth).flatten()\n",
    "    predictions = np.asarray(predictions).flatten()\n",
    "\n",
    "    # Desired states\n",
    "    desired_states = {0, 2}\n",
    "\n",
    "    # Maximum distance in samples (treshold in seconds vs fs)\n",
    "    max_distance_samples = int(threshold * sample_rate)\n",
    "\n",
    "    # Extract runs from ground truth and predictions\n",
    "    gt_runs = extract_state_runs(ground_truth, desired_states)\n",
    "    pred_runs = extract_state_runs(predictions, desired_states)\n",
    "\n",
    "    # Get midpoints and states\n",
    "    gt_midpoints = np.array([run['midpoint'] for run in gt_runs])\n",
    "    gt_states = np.array([run['state'] for run in gt_runs])\n",
    "\n",
    "    pred_midpoints = np.array([run['midpoint'] for run in pred_runs])\n",
    "    pred_states = np.array([run['state'] for run in pred_runs])\n",
    "\n",
    "    # Initialize matches\n",
    "    matched_gt_indices = set()\n",
    "    matched_pred_indices = set()\n",
    "\n",
    "    # Build potential matches\n",
    "    potential_matches = []\n",
    "    for i, (p_mid, p_state) in enumerate(zip(pred_midpoints, pred_states)):\n",
    "        for j, (gt_mid, gt_state) in enumerate(zip(gt_midpoints, gt_states)):\n",
    "            if gt_state == p_state:\n",
    "                distance = abs(p_mid - gt_mid)\n",
    "                if distance <= max_distance_samples:\n",
    "                    potential_matches.append((i, j, distance))\n",
    "\n",
    "    # Sort potential matches by distance\n",
    "    potential_matches.sort(key=lambda x: x[2])\n",
    "\n",
    "    # Perform matching\n",
    "    TP = 0\n",
    "    for i, j, d in potential_matches:\n",
    "        if i not in matched_pred_indices and j not in matched_gt_indices:\n",
    "            matched_pred_indices.add(i)\n",
    "            matched_gt_indices.add(j)\n",
    "            TP += 1\n",
    "\n",
    "    # Compute FP and FN\n",
    "    total_pred = len(pred_midpoints)\n",
    "    total_gt = len(gt_midpoints)\n",
    "    FP = total_pred - len(matched_pred_indices)\n",
    "    FN = total_gt - len(matched_gt_indices)\n",
    "\n",
    "    # Compute PPV and Sensitivity\n",
    "    ppv = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "    return ppv, sensitivity\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have numpy arrays `ground_truth_labels` and `predicted_labels`, and `sample_rate`.\n",
    "\n",
    "# ground_truth_labels = np.array([...])  # Your ground truth labels\n",
    "# predicted_labels = np.array([...])     # Your predicted labels\n",
    "# sample_rate = 1000  # Example sample rate in Hz\n",
    "\n",
    "\n",
    "def compute_schmidt_metrics(ground_truth, sequences, sample_rate):\n",
    "  ppvs, sensitivities, accuracies = [], [], []\n",
    "  for i in range(len(ground_truth)):\n",
    "    ppv, sensitivity = compute_ppv_sensitivity(ground_truth[i],\n",
    "                                               sequences[i],\n",
    "                                               50)\n",
    "    ppvs.append(ppv)\n",
    "    sensitivities.append(sensitivity)\n",
    "    accuracies.append(accuracy_score(ground_truth[i], sequences[i]))\n",
    "  return np.array(ppvs), np.array(sensitivities), np.array(accuracies)\n",
    "\n",
    "\n",
    "ppv, sens, acc = compute_schmidt_metrics(ground_truth, predictions, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(ground_truth, seqs, idx):\n",
    "\n",
    "  # Define the window width in terms of seconds and convert to the corresponding sample range\n",
    "  window_duration = 0.1  # 0.06 ms in seconds\n",
    "  sample_interval = 1 / 50  # Time per sample in seconds (20 ms per sample at 50 Hz)\n",
    "\n",
    "  # Calculate the equivalent width in terms of sample indices (will be <1)\n",
    "  window_width_samples = window_duration / sample_interval\n",
    "\n",
    "  # Create the plot\n",
    "  plt.figure(figsize=(24, 6))\n",
    "\n",
    "  # Plot ground truth and predictions with discrete markers and dotted lines\n",
    "  plt.plot(ground_truth[idx], 'o--', label='Ground Truth', markersize=6)\n",
    "  plt.plot(seqs[idx], 'o--', color='red', label='Predictions', markersize=6)\n",
    "\n",
    "\n",
    "  # Set labels and legend\n",
    "  plt.title(f'Signal at idx {idx} with 0.06 ms reference window')\n",
    "  plt.xlabel('Sample Index')\n",
    "  plt.ylabel('Amplitude')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "visualize_predictions(ground_truth, predictions, idx)\n",
    "ppv[idx], sens[idx], acc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 13\n",
    "visualize_predictions(ground_truth, predictions, idx)\n",
    "ppv[idx], sens[idx], acc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 44\n",
    "visualize_predictions(ground_truth, predictions, idx)\n",
    "ppv[idx], sens[idx], acc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and Standard Deviation of metrics\n",
    "ppv_mean = np.mean(ppv)\n",
    "ppv_stdev = np.std(ppv)\n",
    "sens_mean = np.mean(sens)\n",
    "sens_stdev = np.std(sens)\n",
    "acc_mean = np.mean(acc)\n",
    "acc_stdev = np.std(acc)\n",
    "\n",
    "print(\"\\nppv_mean\", ppv_mean)\n",
    "print(\"\\nppv_stdev\", ppv_stdev)\n",
    "print(\"\\nsens_mean\", sens_mean)\n",
    "print(\"\\nsens_stdev\", sens_stdev)\n",
    "print(\"\\nacc_mean\", acc_mean)\n",
    "print(\"\\nacc_stdev\", acc_stdev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signalProcessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
